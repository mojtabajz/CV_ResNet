{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/csauyong/cnn-cifar/blob/main/CIFAR_10.ipynb\n",
    "\n",
    "# Import PyTorch and other libraries\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Define the device to use (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the CIFAR-10 dataset and split it into train and test sets\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define the CNN model with three convolutional layers and a MLP classifier\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, 3) # First convolutional layer with 32 filters of size 3x3\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2) # Pooling layer with 2x2 window size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3) # Second convolutional layer with 64 filters of size 3x3\n",
    "        self.conv3 = torch.nn.Conv2d(64, 64, 3) # Third convolutional layer with 64 filters of size 3x3\n",
    "        self.fc1 = torch.nn.Linear(64 * 4 * 4, 256) # First fully-connected layer with 256 nodes\n",
    "        self.fc2 = torch.nn.Linear(256, 128) # Second fully-connected layer with 128 nodes\n",
    "        self.fc3 = torch.nn.Linear(128, 10) # Output layer with 10 nodes\n",
    "        self.relu = torch.nn.ReLU() # ReLU activation function\n",
    "        self.softmax = torch.nn.Softmax(dim=1) # Softmax activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x))) # Apply first convolutional layer, then ReLU, then pooling\n",
    "        x = self.pool(self.relu(self.conv2(x))) # Apply second convolutional layer, then ReLU, then pooling\n",
    "        x = self.relu(self.conv3(x)) # Apply third convolutional layer, then ReLU\n",
    "        x = x.view(-1, 64 * 4 * 4) # Flatten the feature map\n",
    "        x = self.relu(self.fc1(x)) # Apply first fully-connected layer, then ReLU\n",
    "        x = self.relu(self.fc2(x)) # Apply second fully-connected layer, then ReLU\n",
    "        x = self.softmax(self.fc3(x)) # Apply output layer, then softmax\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device) # Create the model and move it to the device\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss() # Categorical crossentropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # SGD optimizer\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "for epoch in range(20):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # Print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Test the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
